{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lmbhNgaDPjl"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/mydrive\")"
      ],
      "metadata": {
        "id": "jWN5vWsyDe_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/mydrive/MyDrive/23년 컴퓨터비전'"
      ],
      "metadata": {
        "id": "9k19oyh2Do35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq '/content/mydrive/MyDrive/23년 컴퓨터비전/archive.zip'"
      ],
      "metadata": {
        "id": "3Nn638NkEZDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "YFmyC0evErc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "bdS41hnLFgAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일로부터 영상을 읽어서 data에 저장하기\n",
        "labels = ['PNEUMONIA', 'NORMAL']\n",
        "img_size = 299\n",
        "def get_training_data(data_dir):\n",
        "    data = [] \n",
        "    for label in labels: \n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                img_arr = cv2.cvtColor(img_arr , cv2.COLOR_GRAY2BGR)\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return np.array(data)"
      ],
      "metadata": {
        "id": "QgoSrVVbFmg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "YULf2uikGO7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd chest_xray"
      ],
      "metadata": {
        "id": "RF1AOj2-GQqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "6H0OSq4VGUr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "id": "mHjpZfp2GnEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = get_training_data('./chest_xray/train')\n",
        "test = get_training_data('./chest_xray/test')"
      ],
      "metadata": {
        "id": "97yaNICsGiiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[500][1])"
      ],
      "metadata": {
        "id": "0QMFeHHgGxea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "IyiJSxajGzPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature, label in train:\n",
        "  print(feature)\n",
        "  print(label)"
      ],
      "metadata": {
        "id": "atNGbVmrG3DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#폐렴과 정상 데이터의 갯수 확인하기\n",
        "l = []\n",
        "for i in train:\n",
        "    if(i[1] == 0):\n",
        "        l.append(\"Pneumonia\")\n",
        "    else:\n",
        "        l.append(\"Normal\")"
      ],
      "metadata": {
        "id": "OE8rhakXG53y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for feature, label in train:\n",
        "    x_train.append(feature)\n",
        "    y_train.append(label)\n",
        "\n",
        "for feature, label in test:\n",
        "    x_test.append(feature)\n",
        "    y_test.append(label)"
      ],
      "metadata": {
        "id": "rroaZtlYHCpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 Normalize 0~255  --->   0~1\n",
        "x_train = np.array(x_train) / 255\n",
        "x_test = np.array(x_test) / 255"
      ],
      "metadata": {
        "id": "NoYIkKg5HD1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 딥러닝을 위해 data를 resize\n",
        "x_train = x_train.reshape(-1, img_size, img_size, 3)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = x_test.reshape(-1, img_size, img_size, 3)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "yTaXuNx1HNfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data augmentation하기 (overfitting을 막고, 데이터셋내의 데이터 불균형도 어느 정도 해소)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "pigyglI6HQHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 만들기\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (150,150,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 128 , activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units = 1 , activation = 'sigmoid'))\n",
        "model.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "VbbQ6rstHWzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learning_rate를 줄이는 방법. \n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 5, verbose=1,factor=0.3, min_lr=0.000001)"
      ],
      "metadata": {
        "id": "pXsNlSyVHaER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "inception = InceptionV3(weights='imagenet',input_shape=(299,299,3) , include_top=True)\n",
        "for layer in inception.layers[:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "base_inputs = inception.layers[0].input\n",
        "base_outputs = inception.layers[-2].output\n",
        "classifier = tf.keras.layers.Dense(2)(base_outputs)\n",
        "new_model = keras.Model(inputs=base_inputs, outputs=classifier)\n",
        "\n",
        "\n",
        "new_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "print(new_model.summary())\n",
        "new_model.fit(x_train, y_train, epochs=15)\n",
        "\n",
        "loss, accuracy = new_model.evaluate(x_test, y_test)\n",
        "print(\"Loss of the model is:\", loss)\n",
        "print(\"Accuracy of the model is:\", accuracy * 100, \"%\")"
      ],
      "metadata": {
        "id": "hXWp7AHCI_5d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}